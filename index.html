<meta charset="utf-8">

<meta name="description" content="SPLICE is a part-level neural implicit representation enabling intuitive and structure-aware 3D shape editing, supporting translation, rotation, scaling, deletion, duplication, and cross-shape part mixing with high semantic consistency.">

<meta property="og:title" content="SPLICE: Part-Level 3D Shape Editing"/>
<meta property="og:description" content="SPLICE enables flexible and high-fidelity 3D shape editing through part-level neural implicit representations and global neural mixing."/>
<meta property="og:url" content="URL OF THE WEBSITE"/>
<meta property="og:image" content="static/images/social_preview.png"/>
<meta property="og:image:width" content="1200"/>
<meta property="og:image:height" content="630"/>

<meta name="twitter:title" content="SPLICE: Part-Level 3D Shape Editing">
<meta name="twitter:description" content="A neural implicit framework for flexible part-level 3D shape editing with semantic consistency.">
<meta name="twitter:image" content="static/images/social_preview.png">
<meta name="twitter:card" content="summary_large_image">

<meta name="keywords" content="3D shape editing, neural implicit representation, part-level editing, shape analysis, computer graphics, AI">

<meta name="viewport" content="width=device-width, initial-scale=1">

<title>SPLICE</title>
<link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

<link rel="stylesheet" href="static/css/bulma.min.css">
<link rel="stylesheet" href="static/css/bulma-carousel.min.css">
<link rel="stylesheet" href="static/css/bulma-slider.min.css">
<link rel="stylesheet" href="static/css/fontawesome.all.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="static/css/index.css">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
<script defer src="static/js/fontawesome.all.min.js"></script>
<script src="static/js/bulma-carousel.min.js"></script>
<script src="static/js/bulma-slider.min.js"></script>
<script src="static/js/index.js"></script>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title">
            SPLICE: Part-Level 3D Shape Editing from Local Semantic Extraction to Global Neural Mixing
          </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="" target="_blank">Jin Zhou</a>,</span>
            <span class="author-block">
              <a href="" target="_blank">Hongliang Yang</a>,</span>
            <span class="author-block">
              <a href="https://pengfeixu.com/" target="_blank">Pengfei Xu</a>,</span>
            <span class="author-block">
              <a href="https://vcc.tech/~huihuang" target="_blank">Hui Huang</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">CSSE, Shenzhen University<br>Pacific Graphics 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Main PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2512.04514" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- <!-- Video link -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=YOUR_VIDEO_ID" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-video"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->

              <!-- GitHub link (if you have code, else delete this block) -->
              <!--
              <span class="link-block">
                <a href="https://github.com/YOUR_REPO" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              -->

            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="MRPilot Overview" style="width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-justified" style="margin-top: 20px;">
        <strong>Part-level editing results produced by SPLICE:</strong>  Our method supports a wide range of intuitive editing operations, including sequential edits, copy, move, delete, rotate, scale, and mix, without requiring manual post-adjustment. The edited shapes remain structurally coherent and visually plausible. Additionally, SPLICE exhibits strong robustness under multi-step editing, consistently maintaining high reconstruction quality throughout the editing process.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          People often need guidance to complete tasks with specific requirements or sophisticated steps, such as preparing a meal or assembling furniture. Traditional guidance often relies on unstructured paper instructions that require people to switch between reading instructions and performing actions, resulting in an unsmooth user experience. Recent Mixed Reality (MR) systems alleviate this problem by giving spatialized navigation but demand an authoring step and, therefore, cannot be easily adapted to general tasks. We propose MRPilot, an MR system empowered by Large Language Models (LLMs) and Computer Vision techniques, offering responsive navigation for general tasks without pre-authoring. MRPilot consists of three modules: a Navigation Builder Module using LLMs to generate structured instructions, an Object Anchor Module exploiting Computer Vision techniques to anchor physical objects with virtual proxies, and an Action Recommendation Module giving responsive navigation according to usersâ€™ interactions with physical objects. MRPilot bridges the gap between virtual instructions and physical interactions for general tasks, providing contextual and responsive navigation. We conducted a user study to compare MRPilot with a baseline MR system that also exploited LLMs. The results confirmed the effectiveness of MRPilot.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
